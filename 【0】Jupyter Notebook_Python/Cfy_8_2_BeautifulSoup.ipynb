{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# '''一 、BeautifulSoup4 简介： '''\n",
    "安装：pip install bs4\n",
    "官方文档：https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/#\n",
    "          https://beautifulsoup.readthedocs.io/zh_CN/latest/\n",
    "BeautifulSoup4和 lxml 一样，Beautiful Soup 也是一个HTML/XML的解析器，主要的功能也是如何解析和提取 HTML/XML 数据。\n",
    "BeautifulSoup支持Python标准库中的HTML解析器,还支持一些第三方的解析器，如果我们不安装它，则 Python 会使用\n",
    "Python默认的解析器，lxml 解析器更加强大，速度更快，推荐使用lxml 解析器。\n",
    "Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为utf-8编码。你不需要考虑编码方式，除非文档没有\n",
    "指定一个编码方式，这时，Beautiful Soup·就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。\n",
    "\n",
    "# '''二 、BeautifulSoup4主要解析器，以及优缺点：'''\n",
    "1. Python标准库：BeautifulSoup(markup,'html.parser')\n",
    "2. lxml HTML解析器：BeautifulSoup(markup,'lxml')\n",
    "3. lxml XML解析器：BeautifulSoup(markup,'xml') / BeautifulSoup(markup,'lxml-xml')\n",
    "4. html5lib: BeautifulSoup(markup,'html5lib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5- {'lang': 'eng'}\n",
      "5- True\n",
      "6- eng\n",
      "7- eng\n",
      "7-1 eng\n",
      "8- Harry Potter\n",
      "8-1 Harry Potter\n",
      "9- <class 'bs4.element.NavigableString'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'————————————————————————————————'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （一）BeautifulSoup 基本属性：prettify、get_text、name、attrs、has_attr、title、string、text、get\n",
    "from bs4 import BeautifulSoup\n",
    "'''————————————————————————————————'''\n",
    "text='''<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"eng\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book>\n",
    "  <title lang=\"eng\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>'''\n",
    "bf=BeautifulSoup(text)  # create 对象\n",
    "'''——————————prettify、get_text—————————————'''\n",
    "# print(\"1-\",bf.prettify())    # 按照标准缩进格式输出\n",
    "# print(\"2-\",bf.get_text())   # 会将HTML文档中的所有标签清除,返回一个只包含文字的字符串\n",
    "'''——————————name、attrs、string—————————————'''\n",
    "tag=bf.title       # 获取title标签的所有内容\n",
    "# print(\"3-\",tag)\n",
    "# print(type(tag))        # tag类型\n",
    "# print(\"4-\",tag.name)    # 标签名称\n",
    "print(\"5-\",tag.attrs)   # 标签属性\n",
    "print(\"5-\",tag.has_attr('lang'))   # 标签属性\n",
    "print(\"6-\",tag.attrs[\"lang\"]) #单独获取某个属性 方法1\n",
    "print(\"7-\",bf.title[\"lang\"])  #单独获取某个属性 方法2\n",
    "print(\"7-1\",tag.get(\"lang\").strip())\n",
    "print(\"8-\",tag.string)        # 获取标签的文本内容\n",
    "print(\"8-1\",tag.text)         # 获取标签的文本内容\n",
    "print(\"9-\",type(tag.string))  # 查看数据类型\n",
    "'''————————————————————————————————'''\n",
    "# string='''<p><!-- 这是注释！ --></p>''' \n",
    "# sp=BeautifulSoup(string)\n",
    "# print(\"10-\",sp)          # 输出的内容不包括注释符号\n",
    "# print(\"11-\",sp.p.string) # 去获取标签中是文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）Beautifu`lSoup4 四大对象种·类 ：Tag、NavigableString、BeautifulSoup、Comment（生成本地html）\n",
    "from bs4 import BeautifulSoup\n",
    "'''———————— Tag 标签 ————————'''\n",
    "html='''<!DOCTYPE html>\n",
    "<!--STATUS OK-->\n",
    "<html>\n",
    " <head>\n",
    "  <meta content=\"text/html;charset=utf-8\" http-equiv=\"content-type\"/>\n",
    "  <meta content=\"IE=Edge\" http-equiv=\"X-UA-Compatible\"/>\n",
    "  <meta content=\"always\" name=\"referrer\"/>\n",
    "  <link href=\"https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
    "  <title>\n",
    "   百度一下，你就知道\n",
    "  </title>\n",
    " </head>\n",
    " <body link=\"#0000cc\">\n",
    "  <div id=\"wrapper\">\n",
    "   <div id=\"head\">\n",
    "    <div class=\"head_wrapper\">\n",
    "     <div id=\"u1\">\n",
    "      <a class=\"mnav\" href=\"http://news.baidu.com\" name=\"tj_trnews\">\n",
    "       新闻\n",
    "      </a>\n",
    "      <a class=\"mnav\" href=\"https://www.hao123.com\" name=\"tj_trhao123\">\n",
    "       hao123\n",
    "      </a>\n",
    "      <a class=\"mnav\" href=\"http://map.baidu.com\" name=\"tj_trmap\">\n",
    "       地图\n",
    "      </a>\n",
    "      <a class=\"mnav\" href=\"http://v.baidu.com\" name=\"tj_trvideo\">\n",
    "       视频\n",
    "      </a>\n",
    "      <a class=\"mnav\" href=\"http://tieba.baidu.com\" name=\"tj_trtieba\">贴吧</a>\n",
    "      <a class=\"bri\" href=\"//www.baidu.com/more/\" name=\"tj_briicon\" style=\"display: block;\">\n",
    "       更多产品\n",
    "      </a>\n",
    "     </div>\n",
    "    </div>\n",
    "   </div>\n",
    "  </div>\n",
    " </body>\n",
    "</html>'''\n",
    "'''————————————————————————'''\n",
    "# with open(r'C:\\Users\\hp\\Desktop\\test.html','w') as f:\n",
    "#     f.write(html)\n",
    "'''—————————text 参数———————————————'''\n",
    "# html=html.replace('<br>', '').replace('<br/>', '')\n",
    "html=html.replace('<br>', '')\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "# result=bs.find_all('a',text='贴吧')   # 没有换行符时不受影响\n",
    "# result=bs.find_all(text=['视频','新闻'])  # 有换行符时受影响\n",
    "print(result)\n",
    "'''—————————text + def 方法自定义函数———————————————'''\n",
    "def length_is_two(text):\n",
    "    return text and len(text) == 2\n",
    "t_list = bs.find_all(text=length_is_two)\n",
    "for item in t_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）BeautifulSoup4 四大对象种类 _1：Tag.name/string/get_text()\n",
    "from bs4 import BeautifulSoup\n",
    "file = open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html = file.read()\n",
    "bs = BeautifulSoup(html,\"html.parser\")\n",
    "# print(bs.prettify())\n",
    "'''————————— Tag.name/string/get_text() ———————————————'''\n",
    "# print(bs.title)    # 获取title标签的所有内容\n",
    "# print(bs.title.name)    # 获取title标签的所有内容\n",
    "# print(bs.title.string)    # 获取title标签的所有内容\n",
    "# print(bs.title.get_text())    # 获取title标签的所有内容\n",
    "'''———————————————'''\n",
    "# print(bs.head)     # 获取head标签的所有内容\n",
    "# print(bs.a)        # 获取第一个a标签的所有内容\n",
    "# print(type(bs.a))  # 类型\n",
    "'''————————— Tag get获得、修改、del删除标签属性、attrs———————————————'''\n",
    "# [document] #bs 对象本身比较特殊，它的 name 即为 [document]\n",
    "print(bs.name)              # [document] #bs 对象本身比较特殊，它的 name 即为 [document]\n",
    "print(bs.head.name)         # head #对于其他内部标签，输出的值便为标签本身的名称\n",
    "print(bs.a.attrs)           # 把 a 标签的所有属性打印输出了出来，得到的类型是一个字典。\n",
    "print(bs.a['class'])\n",
    "print(bs.a.get('class'))\n",
    "bs.a['class'] = \"newClass\"  # 可以对这些属性和内容等等进行修改\n",
    "print(bs.a) \n",
    "del bs.a['class']           # 还可以对这个属性进行删除\n",
    "print(bs.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）BeautifulSoup4 四大对象种类 _2：NavigableString、BeautifulSoup、Comment\n",
    "from bs4 import BeautifulSoup\n",
    "file = open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html = file.read()\n",
    "bs = BeautifulSoup(html,\"html.parser\")\n",
    "'''——————————————————————————————'''\n",
    "'''BeautifulSoup对象表示的是一个文档的内容。大部分时候，可以把它当作 Tag 对象，\n",
    "是一个特殊的 Tag，我们可以分别获取它的类型，名称，以及属性'''\n",
    "# print(type(bs.name))\n",
    "# print(bs.name)\n",
    "# print(bs.attrs)\n",
    "'''——————————————————————————————'''\n",
    "# print(bs.title.string)\n",
    "# print(bs.title.get_text())\n",
    "# print(type(bs.title.string))\n",
    "'''——————————————————————————————'''\n",
    "print(bs.a)\n",
    "# 此时不能出现空格和换行符，a标签如下：\n",
    "# <a class=\"mnav\" href=\"http://news.baidu.com\" name=\"tj_trnews\"><!--新闻--></a>\n",
    "print(bs.a.string)\n",
    "print(type(bs.a.string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<title lang=\"eng\">Harry Potter</title>, <title lang=\"eng\">Learning XML</title>]\n",
      "[<title lang=\"eng\">Harry Potter</title>, <title lang=\"eng\">Learning XML</title>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'————直接获取指定标签信息parent、parents、next_sibling、previous_sibling、next_siblings —————'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# （三）BeautifulSoup 函数 _1 :find_all()、find、contents、children、parent、parents、next_sibling、\n",
    "from bs4 import BeautifulSoup\n",
    "'''————————————————————————————————'''\n",
    "text='''<?xml version=\"1.0\" encoding=\"ISO-8859-1\"?>\n",
    "<bookstore>\n",
    "<book>\n",
    "  <title lang=\"eng\">Harry Potter</title>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "<book>\n",
    "  <title lang=\"eng\">Learning XML</title>\n",
    "  <price>39.95</price>\n",
    "</book>\n",
    "</bookstore>'''\n",
    "soup=BeautifulSoup(text)  # create 对象\n",
    "'''————————————find_all() 返回结果是一个列表—————————————'''\n",
    "# print(soup.find_all('title'),end=\"\\n-------\\n\")\n",
    "print(soup.find_all(\"title\",attrs={\"lang\":\"eng\"})) \n",
    "print(soup.find_all(\"title\",lang=\"eng\"))             # 查找title标签 属性lang=eng\n",
    "# print(soup.find_all(\"title\",{\"lang\":\"eng\"}))         # 结果同上\n",
    "# print(soup.find_all([\"title\",\"price\"]))              # 一次获取多个标签\n",
    "# print(soup.find_all(\"title\",lang=\"eng\")[0].get_text())     # 获取指定顺序标签的文本\n",
    "# print(soup.find_all(\"title\",lang=\"eng\")[1].get_text()) \n",
    "'''—————————————find() 直接返回第一个元素—————————————'''\n",
    "# print(soup.find(\"title\"))\n",
    "'''——————————— 直接获取指定标签信息contents、children ———————————'''\n",
    "# end=\"\\n-------\\n\"\n",
    "# print(soup.book,end)              # 获取book节点信息\n",
    "# print(soup.book.contents,end)     # 获取book下的所有子节点\n",
    "# print(soup.book.contents[3],end)  # 获取book下的所有子节点中的某一个节点\n",
    "# for content in soup.book.contents:\n",
    "#     print(\"===\",content)\n",
    "'''———————————'''\n",
    "# print(soup.book.children,end)     # children 生成迭代器\n",
    "# for child in soup.book.children:\n",
    "#     print(\"===\",child) \n",
    "'''————直接获取指定标签信息parent、parents、next_sibling、previous_sibling、next_siblings —————'''\n",
    "# print(soup.title.parent,end)\n",
    "# print(soup.book.parent,end)\n",
    "# for parent in soup.title.parents:      # 注意parent和parents区别\n",
    "#     print(\"===\",parent.name)  \n",
    "# print(soup.title.next_sibling,end)     # 获取该节点的下一个兄弟节点\n",
    "# print(soup.title.previous_sibling,end) # 获取该节点的上一个兄弟节点\n",
    "# print(soup.title.next_siblings,end)    # 获取该节点的全部兄弟节点\n",
    "# for i in soup.title.next_siblings: \n",
    "#     print(\"===\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （四）BeautifulSoup 遍历：contents、children、descendants、.strings/.stripped_strings、.parent/.parents及其他\n",
    "from bs4 import BeautifulSoup\n",
    "file = open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html=file.read()\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "'''——————— tag.content 属性可以将tag的子节点以列表的方式输出 ————————————'''\n",
    "# print(bs.head.contents)              \n",
    "# print(bs.head.contents[1])           # 用列表索引来获取它的某一个元素\n",
    "'''———————tag.children：获取Tag的所有子节点，返回一个生成器 ————————————'''\n",
    "# for child in  bs.html.children:\n",
    "#     print(child)\n",
    "'''———————tag.descendants：获取Tag的所有子孙节点 ————————————'''\n",
    "# for descendant in  bs.body.descendants:\n",
    "#     print(descendant.name)\n",
    "'''———————tag.strings/.stripped_strings：获取Tag的所有子孙节点字符串————————'''\n",
    "# for string in  bs.body.strings:           # 保留了较多空白内容\n",
    "# for string in  bs.body.stripped_strings:  # 除掉那些多余的空白内容\n",
    "#     print(string)\n",
    "'''———————tag.parent/.parents：获取Tag的所有子孙节点字符串————————'''\n",
    "# print(bs.body.parent.name)\n",
    "# for each in bs.body.parents:\n",
    "#     print(each.name)\n",
    "'''——————— 其他属性 ———————'''\n",
    ".previous_sibling：获取当前Tag的上一个节点，属性通常是字符串或空白，真实结果是当前标签与上一个标签之间的顿号和换行符\n",
    ".next_sibling：获取当前Tag的下一个节点，属性通常是字符串或空白，真是结果是当前标签与下一个标签之间的顿号与换行符\n",
    ".previous_siblings：获取当前Tag的上面所有的兄弟节点，返回一个生成器\n",
    ".next_siblings：获取当前Tag的下面所有的兄弟节点，返回一个生成器\n",
    ".previous_element：获取解析过程中上一个被解析的对象(字符串或tag)，可能与previous_sibling相同，但通常是不一样的\n",
    ".next_element：获取解析过程中下一个被解析的对象(字符串或tag)，可能与next_sibling相同，但通常是不一样的\n",
    ".previous_elements：返回一个生成器，可以向前访问文档的解析内容\n",
    ".next_elements：返回一个生成器，可以向后访问文档的解析内容\n",
    ".has_attr：判断Tag是否包含属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （五）BeautifulSoup 搜索 _1 :find_all()、find、re、get_text、get、text、strip 、string\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "<p class=\"story\">...</p></body></html>\"\"\"\n",
    "#创建一个BeautifulSoup解析对象\n",
    "soup = BeautifulSoup(html_doc,\"html.parser\",from_encoding=\"utf-8\")\n",
    "'''—————————— find_all 结果返回所有符合条件的一个列表 ——————————'''\n",
    "print(\"所有的链接\")\n",
    "links = soup.find_all('a')  \n",
    "print(\"1\",links[0])\n",
    "print(\"1.1\",links[0].string)\n",
    "print(\"2\",links[0].text)\n",
    "print(\"2.1\",links[0].text.strip())\n",
    "print(\"3\",links[0].get('href').strip())\n",
    "print(\"3.1\",links[0].get('href'))\n",
    "print(\"4\",links[0].get_text())\n",
    "print(\"5\",links) \n",
    "for link in links:\n",
    "    print(link.name,link['href'],link.get_text())\n",
    "\n",
    "# print(\"获取所有P段落的文字\")\n",
    "# p_nodeall = soup.find_all('p',class_=\"story\")\n",
    "# for each in p_nodeall:\n",
    "#     print(each.name,each['class'],each.get_text())\n",
    "\n",
    "# p_nodeall = soup.find_all('p')\n",
    "# print(\"获取指定P段落的文字\")\n",
    "# print(p_nodeall[1].name,p_nodeall[1]['class'],p_nodeall[1].get_text())\n",
    "# '''———————————— find 结果返回的是第一个符合条件的——————————————'''\n",
    "# print(\"获取特定的URL地址\")     \n",
    "# link_node = soup.find('a',href=\"http://example.com/elsie\")\n",
    "# print(link_node.name,link_node['href'],link_node['class'],link_node.get_text())\n",
    "\n",
    "# print(\"获取第一个P段落的文字\")\n",
    "# p_node = soup.find('p',class_='story')\n",
    "# print(p_node.name,p_node['class'],p_node.get_text())\n",
    "# '''———————————— re 结果返回所有符合条件的结果——————————————'''\n",
    "# print(\"正则表达式匹配\")\n",
    "# link_node = soup.find('a',href=re.compile(r\"ti\"))\n",
    "# print(link_node.name,link_node['href'],link_node['class'],link_node.get_text())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （五）BeautifulSoup 搜索_2 ：find_all、has_attr、re.compile、def、字典、text、limit限制返回的数量\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "file=open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html=file.read()\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "'''———————传入一个正则表达式———————————————'''\n",
    "# result=bs.find_all(re.compile(r'a'))\n",
    "# result=bs.find_all('a',cabslass_=re.compile(r'mnav'))\n",
    "# result= bs.find_all(href=re.compile(\"http://news.baidu.com\"))  # 查询href属性包含ss1.bdstatic.com的Tag\n",
    "'''———————传入一个列表———————————————'''\n",
    "# result=bs.find_all([\"meta\",\"link\"])\n",
    "'''———————传入一个方法，根据方法来匹配———————————————'''\n",
    "# def name_is_exists(tag):\n",
    "#     return tag.has_attr(\"name\")\n",
    "# result = bs.find_all(name_is_exists)\n",
    "'''——————————包含class的Tag属于关键字，所以加_以示区别————————————'''\n",
    "# 查询所有包含class的Tag(注意：class在Python中属于关键字，所以加_以示区别)\n",
    "# result = bs.find_all(class_=True)\n",
    "# print(result)\n",
    "'''—————使用attrs参数，定义一个 字典 来搜索包含特殊属性的tag——————————'''  \n",
    "# result=bs.find_all(attrs={\"id\":\"head\"})\n",
    "'''————通过text参数可以搜索文档中的字符串内容（会受换行符影响），字符串，正则表达式，列表—————'''\n",
    "# result= bs.find_all(text=re.compileabs(\"(地图)\"))\n",
    "# result = bs.find_all(text=re.compile(\"\\d\"))\n",
    "'''——————limit参数来限制返回的数量—————————'''  \n",
    "# result = bs.find_all(\"a\",limit=2)\n",
    "'''——————————简写————————————''' \n",
    "# result=bs('a')\n",
    "'''——————————————————————'''  \n",
    "for rr in result:\n",
    "    print(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （五）BeautifulSoup 搜索_3 ：find\n",
    "'''find_all，尽管传入了limit=1，但是返回值仍然为一个列表，当我们只需要取一个值时，远不如find方法方便。但\n",
    "是如果未搜索到值时，将返回一个None.我们知道可以通过bs.div来获取第一个div标签，如果我们需要获取第一个div下\n",
    "的第一个div，'''\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "file=open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html=file.read()\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "'''——————————————————————————'''\n",
    "# t = bs.div.absdiv\n",
    "# t = bs.find(\"div\").find(\"div\")\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （六）BeautifulSoup CSS选择器：select 通过标签名/类名/id/组合/属性/子标签/兄弟节点标签查找、获取内容\n",
    "'''BeautifulSoup支持发部分的CSS选择器，在Tag获取BeautifulSoup对象的\n",
    ".select()方法中传入字符串参数，即可使用CSS选择器的语法找到Tag:'''\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "file=open(r'C:\\Users\\hp\\Desktop\\test.html', 'rb')\n",
    "html=file.read()\n",
    "bs=BeautifulSoup(html,'html.parser')\n",
    "'''————————— 通过标签名查找 ———————————'''\n",
    "# print(bs.select('title'))\n",
    "# print(bs.select('a'))\n",
    "'''————————— 通过类名查找 ———————————'''\n",
    "# print(bs.select('.mnav'))\n",
    "'''————————— 通过id查找 ———————————'''\n",
    "# print(bs.select('#u1'))\n",
    "'''————————— 组合查找 ———————————'''\n",
    "# print(bs.select('div .bri'))\n",
    "'''————————— 属性查找 ———————————'''\n",
    "# print(bs.select('a[class=\"bri\"]'))\n",
    "# print(bs.select('a[href=\"http://tieba.baidu.com\"]'))\n",
    "'''————————— 子标签查找 ———————————'''\n",
    "# print(bs.select(\"head > title\"))\n",
    "# print(bs.select(\"div > a \"))\n",
    "'''————————— 兄弟节点标签查找 ———————————'''\n",
    "# print(bs.select(\".mnav ~ .bri\"))\n",
    "'''————————— 获取内容 ———————————'''\n",
    "# print(bs.select(\"title\"))\n",
    "# print(bs.select('title')[0].get_text())\n",
    "# print(bs.select(\"a\"))\n",
    "# print(bs.select('a')[0].get_text())\n",
    "# print(bs.select('a')[1].string)\n",
    "'''__________________________________________________________________'''\n",
    "# products = soup.select('li.rank-item')   # 标签+ .Classic属性\n",
    "# for product in products:\n",
    "#     rank = product.select('div.num')[0].text\n",
    "#     name = product.select('div.info > a')[0].text.strip() # 标签+Classic属性+ 子标签 a\n",
    "#     play = product.select('span.data-box')[0].text\n",
    "#     comment = product.select('span.data-box')[1].text\n",
    "#     up = product.select('span.data-box')[2].text\n",
    "#     url = product.select('div.info > a')[0].attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['现在的温度：0 现在天气情况：阴']\n"
     ]
    }
   ],
   "source": [
    "# （七）BeautifulSoup(lxml) + requests ——1天气实例： —— https://tianqi.so.com/ \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def get_Data(url):\n",
    "    data_list = []\n",
    "    response = requests.get(url)\n",
    "    print(response.status_code)\n",
    "#     html_doc = response.text                      # 获得页面内容\n",
    "    html_doc=response.content.decode('utf-8')     # 获得页面内容\n",
    "#     soup = BeautifulSoup(html_doc, 'html.parser')       # 自动补全html代码，并按html代码格式返回\n",
    "    soup = BeautifulSoup(html_doc, 'lxml')              # 自动补全html代码，并按html代码格式返回\n",
    "    wendu = soup.find('div', class_='temperature').get_text()    # 获得标签文本\n",
    "#     wendu = soup.find('div', class_='temperature').string        # 获得标签文本\n",
    "\n",
    "    tianqi = soup.find('div', class_='weather-icon-wrap').get_text()\n",
    "    data_list.append(\"现在的温度：%s 现在天气情况：%s\" % (wendu, tianqi))\n",
    "    print(data_list)\n",
    "if __name__ == '__main__':\n",
    "    url1 = 'https://tianqi.so.com/weather/'\n",
    "    get_Data(url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# （八）BeautifulSoup(lxml) + requests ——下载漫画（代码分析）：——https://www.dmzj.com/\n",
    "''' https://mp.weixin.qq.com/s/oo3d-Iz7vAAJ5M3Ecm17vw '''\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "'''________________________ 获取章节名和章节链接 _______________________________'''\n",
    "# target_url = \"https://www.dmzj.com/info/yaoshenji.html\"\n",
    "# r = requests.get(url=target_url)\n",
    "# bs = BeautifulSoup(r.text, 'lxml')\n",
    "# list_con_li = bs.find('ul', class_=\"list_con_li\")\n",
    "# comic_list = list_con_li.find_all('a')\n",
    "# chapter_names = []\n",
    "# chapter_urls = []\n",
    "# n=0\n",
    "# for comic in comic_list:\n",
    "#     href = comic.get('href')\n",
    "#     name = comic.text\n",
    "#     chapter_names.insert(0, name)\n",
    "#     chapter_urls.insert(0, href)\n",
    "#     n=n+1\n",
    "#     if n==10 :\n",
    "#         break\n",
    "# print(chapter_names)\n",
    "# print(chapter_urls)\n",
    "'''__________________ 获取漫画图片地址(动态加载：内部加载与外部加载) 图片地址乱序_____________________'''\n",
    "# 使用view-source:方法，就是看页面源码，并不管动态加载的内容。这里面没有图片链接，就说明图片是动态加载的。\n",
    "# 外部加载就是在html页面中，以引用的形式，加载一个js，例如这样：\n",
    "# <script type=\"text/javascript\" src=\"https://cuijiahua.com/call.js\"></script>\n",
    "# 这段代码得意思是，引用cuijiahua.com域名下的call.js文件。\n",
    "\n",
    "# 内部加载就是Javascript脚本内容写在html内，例如这个漫画网站。\n",
    "# https://images.dmzj1.com/img/chapterpic/3059/14237/14395217739069.jpg\n",
    "# 图片链接是这个，那就用图片的名字去掉后缀，也就是14395217739069在浏览器的调试页面搜索，\n",
    "# 因为一般这种动态加载，链接都是程序合成的，搜它准没错\n",
    "\n",
    "# url = 'https://www.dmzj.com/view/yaoshenji/41917.html'\n",
    "# r = requests.get(url=url)\n",
    "# html = BeautifulSoup(r.text, 'lxml')\n",
    "# script_info = html.script\n",
    "# pics = re.findall('\\d{13,14}', str(script_info))\n",
    "# chapterpic_hou = re.findall('\\|\\|(\\d{5})', str(script_info))[0]\n",
    "# chapterpic_qian = re.findall('\\|jpg\\|(\\d{4})', str(script_info))[0]\n",
    "# for pic in pics:\n",
    "#     url = 'https://images.dmzj1.com/img/chapterpic/' + chapterpic_qian + '/' + chapterpic_hou + '/' + pic + '.jpg'\n",
    "#     print(url)\n",
    "'''__________________ 获取漫画图片地址(动态加载：内部加载与外部加载) 图片地址排序：_____________________'''\n",
    "# url = 'https://www.dmzj.com/view/yaoshenji/41917.html'\n",
    "# r = requests.get(url=url)\n",
    "# html = BeautifulSoup(r.text, 'lxml')\n",
    "# script_info = html.script\n",
    "# pics = re.findall('\\d{13,14}', str(script_info))\n",
    "# for idx, pic in enumerate(pics):\n",
    "#     if len(pic) == 13:\n",
    "#         pics[idx] = pic + '0'\n",
    "# pics = sorted(pics, key=lambda x:int(x))\n",
    "# chapterpic_hou = re.findall('\\|\\|(\\d{5})', str(script_info))[0]\n",
    "# chapterpic_qian = re.findall('\\|jpg\\|(\\d{4})', str(script_info))[0]\n",
    "# for pic in pics:\n",
    "#     if pic[-1] == '0':\n",
    "#         url = 'https://images.dmzj1.com/img/chapterpic/' + chapterpic_qian + '/' + chapterpic_hou + '/' + pic[:-1] + '.jpg'\n",
    "#     else:\n",
    "#         url = 'https://images.dmzj1.com/img/chapterpic/' + chapterpic_qian + '/' + chapterpic_hou + '/' + pic + '.jpg'\n",
    "#     print(url)\n",
    "'''__________________from urllib.request import urlretrieve 下载图片：因反爬虫失败 _________________'''\n",
    "# from urllib.request import urlretrieve\n",
    "# dn_url = 'https://images.dmzj.com/img/chapterpic/3059/14237/1439521788936.jpg'\n",
    "# urlretrieve(dn_url,'C:/Users/hp/Desktop/1.jpg')\n",
    "# 这个地址就是图片的真实地址，在浏览器中打开，可能直接无法打开，或者能打开，但是一刷新就又不能打开了！\n",
    "# 典型的通过Referer的反扒爬虫手段！\n",
    "# Referer可以理解为来路，先打开章节URL链接，再打开图片链接。打开图片的时候，Referer的信息里保存的是章节URL。\n",
    "# 动漫之家网站的做法就是，站内的用户访问这个图片，我就给他看，从其它地方过来的用户，我就不给他看。\n",
    "# 是不是站内用户，就是根据Referer进行简单的判断。\n",
    "\n",
    "'''__________________from contextlib import closing 解决下载图片：因反爬虫失败 ___________________'''\n",
    "# 使用closing方法可以设置Headers信息，这个Headers信息里保存Referer来路，\n",
    "# 就是第一章的URL，最后以写文件的形式，保存这个图片。\n",
    "from contextlib import closing\n",
    "download_header = {'Referer': 'https://www.dmzj.com/view/yaoshenji/41917.html'}\n",
    "dn_url = 'https://images.dmzj1.com/img/chapterpic/3059/14237/14395217940216.jpg'\n",
    "with closing(requests.get(dn_url, headers=download_header, stream=True)) as response:\n",
    "    chunk_size = 1024  \n",
    "    content_size = int(response.headers['content-length'])  \n",
    "    if response.status_code == 200:\n",
    "        print('文件大小:%0.2f KB' % (content_size / chunk_size))\n",
    "        with open('C:/Users/hp/Desktop/11.jpg', \"wb\") as file:  \n",
    "            for data in response.iter_content(chunk_size=chunk_size):  \n",
    "                file.write(data)  \n",
    "    else:\n",
    "        print('链接异常')\n",
    "print('下载完成！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （八）BeautifulSoup(lxml) + requests + contextlib + tqdm ——下载漫画（代码整合）：——https://www.dmzj.com/\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from contextlib import closing\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# 创建保存目录\n",
    "save_dir = '妖神记'\n",
    "if save_dir not in os.listdir('./'):\n",
    "    os.mkdir(save_dir)\n",
    "target_url = \"https://www.dmzj.com/info/yaoshenji.html\"\n",
    "\n",
    "# 获取动漫章节链接和章节名\n",
    "r = requests.get(url = target_url)\n",
    "bs = BeautifulSoup(r.text, 'lxml')\n",
    "list_con_li = bs.find('ul', class_=\"list_con_li\")\n",
    "cartoon_list = list_con_li.find_all('a')\n",
    "chapter_names = []\n",
    "chapter_urls = []\n",
    "n=0\n",
    "for cartoon in cartoon_list:\n",
    "    href = cartoon.get('href')\n",
    "    name = cartoon.text\n",
    "    chapter_names.insert(0, name)\n",
    "    chapter_urls.insert(0, href)\n",
    "    n=n+1\n",
    "    if n==2 :\n",
    "        break\n",
    "\n",
    "# 下载漫画 \n",
    "for i, url in enumerate(tqdm(chapter_urls)):\n",
    "    download_header = {'Referer': url}\n",
    "    name = chapter_names[i]\n",
    "    # 去掉.\n",
    "    while '.' in name:\n",
    "        name = name.replace('.', '')\n",
    "    chapter_save_dir = os.path.join(save_dir, name)\n",
    "    if name not in os.listdir(save_dir):\n",
    "        os.mkdir(chapter_save_dir)\n",
    "        r = requests.get(url = url)\n",
    "        html = BeautifulSoup(r.text, 'lxml')\n",
    "        script_info = html.script\n",
    "        pics = re.findall('\\d{13,14}', str(script_info))\n",
    "        for j, pic in enumerate(pics):\n",
    "            if len(pic) == 13:\n",
    "                pics[j] = pic + '0'\n",
    "        pics = sorted(pics, key=lambda x:int(x))\n",
    "        chapterpic_hou = re.findall('\\|(\\d{5})\\|', str(script_info))[0]\n",
    "        chapterpic_qian = re.findall('\\|(\\d{4})\\|', str(script_info))[0]\n",
    "        for idx, pic in enumerate(pics):\n",
    "            if pic[-1] == '0':\n",
    "                url = 'https://images.dmzj1.com/img/chapterpic/' + chapterpic_qian + '/' + chapterpic_hou + '/' + pic[:-1] + '.jpg'\n",
    "            else:\n",
    "                url = 'https://images.dmzj1.com/img/chapterpic/' + chapterpic_qian + '/' + chapterpic_hou + '/' + pic + '.jpg'\n",
    "            pic_name = '%03d.jpg' % (idx + 1)\n",
    "            pic_save_path = os.path.join(chapter_save_dir, pic_name)\n",
    "            with closing(requests.get(url, headers = download_header, stream = True)) as response:  \n",
    "                chunk_size = 1024  \n",
    "                content_size = int(response.headers['content-length'])  \n",
    "                if response.status_code == 200:\n",
    "                    with open(pic_save_path, \"wb\") as file:  \n",
    "                        for data in response.iter_content(chunk_size=chunk_size):  \n",
    "                            file.write(data)  \n",
    "                else:\n",
    "                    print('链接异常')\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# （九）BeautifulSoup(lxml) + requests——string、find、findall、strip、get下载小说：https://www.xsbiquge.com/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "'''________________  获取第一章小说内容 __________________'''\n",
    "# if __name__ == '__main__':\n",
    "#     target = 'https://www.xsbiquge.com/15_15338/8549128.html'\n",
    "#     req = requests.get(url = target)\n",
    "#     req.encoding = 'utf-8'\n",
    "#     html = req.text\n",
    "#     bs = BeautifulSoup(html, 'lxml')\n",
    "#     texts = bs.find('div', id='content') \n",
    "#     print(texts.text.strip().split('\\xa0'*4)) \n",
    "# texts.text 是提取所有文字，然后再使用 strip 方法去掉回车，\n",
    "# 最后使用 split 方法根据 \\xa0 切分数据，因为每一段的开头，都有四个空格。\n",
    "\n",
    "'''________________  获取整本小说：获取章节链接 __________________'''\n",
    "# if __name__ == '__main__':\n",
    "#     server = 'https://www.xsbiquge.com'\n",
    "#     target = 'https://www.xsbiquge.com/15_15338/'\n",
    "#     req = requests.get(url = target)\n",
    "#     req.encoding = 'utf-8'\n",
    "#     html = req.text\n",
    "#     bs = BeautifulSoup(html, 'lxml')\n",
    "#     chapters = bs.find('div', id='list')\n",
    "#     chapters = chapters.find_all('a')\n",
    "#     for chapter in chapters:\n",
    "#         url = chapter.get('href')\n",
    "# #         url = chapter['href']\n",
    "#         print(chapter.string)\n",
    "# #         print(chapter.text)\n",
    "# #         print(chapter.get_text())\n",
    "#         print(server + url)\n",
    "'''________________  获取整本小说：整合代码保存为txt __________________'''\n",
    "def get_content(target):\n",
    "    req = requests.get(url = target)\n",
    "    req.encoding = 'utf-8'\n",
    "    html = req.text\n",
    "    bf = BeautifulSoup(html, 'lxml')\n",
    "    texts = bf.find('div', id='content')\n",
    "    content = texts.text.strip().split('\\xa0'*4)\n",
    "    return content\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    server = 'https://www.xsbiquge.com'\n",
    "    book_name = '诡秘之主.txt'\n",
    "    target = 'https://www.xsbiquge.com/15_15338/'\n",
    "    req = requests.get(url = target)\n",
    "    req.encoding = 'utf-8'\n",
    "    html = req.text\n",
    "    chapter_bs = BeautifulSoup(html, 'lxml')\n",
    "    chapters = chapter_bs.find('div', id='list')\n",
    "    chapters = chapters.find_all('a')\n",
    "    for chapter in tqdm(chapters):\n",
    "        chapter_name = chapter.string\n",
    "        url = server + chapter.get('href')\n",
    "        content = get_content(url)\n",
    "        with open('C:\\\\Users\\\\hp\\\\Desktop\\\\'+ book_name, 'a', encoding='utf-8') as f:\n",
    "            f.write(chapter_name)\n",
    "            f.write('\\n')\n",
    "            f.write('\\n'.join(content))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# （十）BeautifulSoup(lxml) + requests——post搜索并下载视频：ok资源采集网 http://www.jisudhw.com/\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import ffmpy3\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "'''________________ （1）搜索资源+获得资源详情页链接  ____________________'''\n",
    "# search_keyword = '越狱第一季'\n",
    "# search_url = 'http://www.jisudhw.com/index.php'\n",
    "# serach_params = {'m': 'vod-search'}\n",
    "# serach_headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36',\n",
    "#     'Referer': 'http://www.jisudhw.com/',\n",
    "#     'Origin': 'http://www.jisudhw.com',\n",
    "#     'Host': 'www.jisudhw.com'}\n",
    "# serach_datas = { 'wd': search_keyword,'submit': 'search'}\n",
    "# video_dir = ''\n",
    "# r = requests.post(url=search_url, params=serach_params, headers=serach_headers, data=serach_datas)\n",
    "# r.encoding = 'utf-8'\n",
    "# server = 'http://www.jisudhw.com'\n",
    "# search_html = BeautifulSoup(r.text, 'lxml')\n",
    "# search_spans = search_html.find_all('span', class_='xing_vb4')\n",
    "# for span in search_spans:\n",
    "#     url = server + span.a.get('href')\n",
    "#     name = span.a.string\n",
    "#     print(name)\n",
    "#     print(url)\n",
    "#     video_dir = name\n",
    "#     if name not in os.listdir('./'):\n",
    "#         os.mkdir(name)\n",
    "'''_________________________（2）解析详情页，获得下载链接 _________________________________________'''\n",
    "# detail_url = 'http://www.jisudhw.com/?m=vod-detail-id-15409.html'\n",
    "# r = requests.get(url = detail_url)\n",
    "# r.encoding = 'utf-8'\n",
    "# detail_bf = BeautifulSoup(r.text, 'lxml')\n",
    "# num = 1\n",
    "# serach_res = {}\n",
    "# for each_url in detail_bf.find_all('input'):\n",
    "#     if 'm3u8' in each_url.get('value'):\n",
    "#         url = each_url.get('value')\n",
    "#         if url not in serach_res.keys():\n",
    "#             serach_res[url] = num\n",
    "#         print('第%03d集:' % num)\n",
    "#         print(url)\n",
    "#         num += 1\n",
    "'''_________________________（3）下载一集电视剧 _________________________________________'''\n",
    "url=\"http://youku.com-youku.net/20180614/11920_4c9e1cc1/index.m3u8\"\n",
    "def downVideo(url):\n",
    "    name = \"C:\\\\Users\\\\hp\\\\Desktop\\\\第1集.mp4\"\n",
    "    ffmpy3.FFmpeg(inputs={url: None}, outputs={name:None}).run()         \n",
    "# 开8个线程池\n",
    "pool = ThreadPool(8)\n",
    "results = pool.map(downVideo(url), serach_res.keys())\n",
    "pool.close()\n",
    "pool.join()\n",
    "'''_________________________（4）代码整合在一起，开始下载电视剧 _________________________________________'''\n",
    "# search_keyword = '越狱第一季'\n",
    "# search_url = 'http://www.jisudhw.com/index.php'\n",
    "# serach_params = {'m': 'vod-search'}\n",
    "# serach_headers = {\n",
    "#     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36',\n",
    "#     'Referer': 'http://www.jisudhw.com/', 'Origin': 'http://www.jisudhw.com', 'Host': 'www.jisudhw.com'}\n",
    "# serach_datas = {'wd': search_keyword,'submit': 'search'}\n",
    "# video_dir = ''   \n",
    "# r = requests.post(url=search_url, params=serach_params, headers=serach_headers, data=serach_datas)\n",
    "# r.encoding = 'utf-8'\n",
    "# server = 'http://www.jisudhw.com'\n",
    "# search_html = BeautifulSoup(r.text, 'lxml')\n",
    "# search_spans = search_html.find_all('span', class_='xing_vb4')\n",
    "# for span in search_spans:\n",
    "#     url = server + span.a.get('href')\n",
    "#     name = span.a.string\n",
    "#     print(name)\n",
    "#     print(url)\n",
    "#     video_dir = name\n",
    "#     if name not in os.listdir('./'):\n",
    "#         os.mkdir(name)\n",
    "#     detail_url = url\n",
    "#     r = requests.get(url = detail_url)\n",
    "#     r.encoding = 'utf-8'\n",
    "#     detail_bf = BeautifulSoup(r.text, 'lxml')\n",
    "#     num = 1\n",
    "#     serach_res = {}\n",
    "#     for each_url in detail_bf.find_all('input'):\n",
    "#         if 'm3u8' in each_url.get('value'):\n",
    "#             url = each_url.get('value')\n",
    "#             if url not in serach_res.keys():\n",
    "#                 serach_res[url] = num\n",
    "#             print('第%03d集:' % num)\n",
    "#             print(url)\n",
    "#             num += 1\n",
    "'''_________________________________'''\n",
    "# def downVideo(url):\n",
    "#     num = serach_res[url]\n",
    "#     name = os.path.join(video_dir, '第%03d集.mp4' % num)\n",
    "#     ffmpy3.FFmpeg(inputs={url: None}, outputs={name:None}).run()         \n",
    "# # 开8个线程池\n",
    "# pool = ThreadPool(8)\n",
    "# results = pool.map(downVideo, serach_res.keys())\n",
    "# pool.close()\n",
    "# pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （十一）BeautifulSoup(html.parser) +Select CSS +csv选择器 :B站视频热搜榜单 https://www.bilibili.com/ranking?\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "url = 'https://www.bilibili.com/v/popular/rank/all'\n",
    "page = requests.get(url)\n",
    "print(page.status_code)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "title = soup.title.text \n",
    "print(title)\n",
    "# results=soup.find_all('a',class_)\n",
    "# for result in results:\n",
    "#     print(result.text)\n",
    "all_products = []\n",
    "products = soup.select('li.rank-item')   # 标签+ .Classic属性\n",
    "for product in products:\n",
    "    rank = product.select('div.num')[0].text\n",
    "    name = product.select('div.info > a')[0].text.replace('\\r\\n', '').strip()  # 标签+Classic属性+ 子标签 a\n",
    "    play = product.select('span.data-box')[0].text.replace('\\r\\n', '').strip() # 去掉回车+换行+空格\n",
    "    comment = product.select('span.data-box')[1].text.replace('\\r\\n', '').strip()\n",
    "    up = product.select('span.data-box')[2].text.replace('\\r\\n', '').strip()\n",
    "    url = product.select('div.info > a')[0].attrs['href'].replace('\\r\\n', '').strip()\n",
    "\n",
    "    all_products.append({\n",
    "        \"视频排名\":rank,\n",
    "        \"视频名\": name,\n",
    "        \"播放量\": play,\n",
    "        \"弹幕量\": comment,\n",
    "        \"up主\": up,\n",
    "        \"视频链接\": url})\n",
    "# print(all_products)\n",
    "keys = all_products[0].keys()\n",
    "with open(r'C:\\Users\\hp\\Desktop\\1_B站视频热榜TOP100.csv', 'w', newline='', encoding='utf-8-sig') as f:\n",
    "    dict_writer = csv.DictWriter(f, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(all_products)\n",
    "\n",
    "pd.DataFrame(all_products,columns=keys).to_csv(r'C:\\Users\\hp\\Desktop\\2_B站视频热榜TOP100.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\r\n",
      "name\r\n",
      "is\r\n",
      "Tom\n",
      "my name is Tom\n"
     ]
    }
   ],
   "source": [
    "myStr = 'my name is Tom'\n",
    "myStr1='''my           \n",
    "        name          \n",
    "        is          \n",
    "        Tom        '''\n",
    "myStr = myStr.replace(' ','\\r\\n')\n",
    "print(myStr)\n",
    "myStr = myStr.replace('\\r\\n',' ')\n",
    "print(myStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "file = open(r'C:\\Users\\hp\\Desktop\\demo2.html', 'rb')\n",
    "html = file.read()\n",
    "bs = BeautifulSoup(html,\"html.parser\")\n",
    "print(bs.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
