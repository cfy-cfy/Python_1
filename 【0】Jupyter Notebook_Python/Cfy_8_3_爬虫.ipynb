{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 【0】Python爬虫架构：\n",
    "'''\n",
    "网页解析器：将一个网页字符串进行解析，可以按照我们的要求来提取出我们有用的信息，也可以根据DOM树的解析方式来解析。\n",
    "            网页解析器有:\n",
    "            (1)正则表达式（直观，将网页转成字符串通过模糊匹配的方式来提取有价值的信息， 当文档比较复杂的时候，\n",
    "            该方法提取数据的时候就会非常的困难）、\n",
    "            (2) html.parser（Python自带的）、\n",
    "            (3) beautifulsoup（第三方插件）、\n",
    "            (4) lxml（第三方插件，可以解析 xml 和 HTML）\n",
    "            html.parser 和 beautifulsoup 以及 lxml 都是以 DOM 树的方式进行解析的。\n",
    "           （5）Nutch、pyspider、scrap\n",
    "Python爬虫10步走\n",
    "基础知识。包括python基本语法，html常识，重点理解python字符串处理常用方法和2种数据结构：dict（字典）和json。\n",
    "3个IDE：Idle，Sublime，Pycharm, Jupyter notebook , Vscode , notepad++ ,spyder \n",
    "2个爬虫库：urllib 和requests。\n",
    "3种解析方法：beautifufsoup，lxml.etree.xpath，re\n",
    "            lxml.estree-xpath适用于结构规律性较强的网页；\n",
    "            re采用正则匹配，只要能找出唯一适用的正则表达式即可；\n",
    "            beautifulsoup使用也较为简单\n",
    "2种抓包分析方法：F12和fiddler。各浏览器F12后都会进入开发者工具，既可以查看具体的html源码，\n",
    "            也能分析浏览器和服务器端交互过程，包括request方法，headers，cookie，post data等等，\n",
    "            需熟练运用。当面对一些复杂页面或者移动端数据（例如手机APP）时，简单的F12工具则不足以满足需求，\n",
    "            此时可以借助抓包工具Fiddler，设置好代理后就能就能将移动端所有数据交互过程全部记录下来，便于分析和抓取。\n",
    "2个数据库：MySQL和MongoDB。爬虫数据量较大时，简单的存储至本地文件不是一个理想的选择，\n",
    "            这时就可视具体抓取数据格式决定选用其中一种数据库。\n",
    "爬虫身份证：cookie。主要是应对一些必须要登录后方可爬取数据的平台，如知乎、淘宝等，这时就必须\n",
    "            携带一个登录后的cookie。cookie是一个字典格式的数据，简单的可直接复制就能使用，\n",
    "            复杂的可以解析post data后构造，甚至是应用selenium模拟登录后得到。\n",
    "所见即所得：selenium。为了得到爬虫的身份cookie，一些网站的post data又进行了复杂的加密，它几乎可以完全模拟人工登录\n",
    "            的过程，从而实现指哪抓哪。若是用可视的浏览器模式，则可以看到浏览器自动化操作的场面；\n",
    "            若是用headless模式，则是通常意义的爬虫模式。\n",
    "程式化爬虫框架：scrapy。若要进行全站抓取，应用scrapy爬虫框架可以事半功倍。实际上，scrapy是一个爬虫模板，\n",
    "            通过定制其中的一些设置后（如爬取初始页，抓取数据格式，抓取方法等）即可以快速实现全站抓取。\n",
    "            scrapy用得熟练之后，也十分有利于培养爬虫工程师思维。\n",
    "高效爬虫：threading。就像计算机最终从单核走向了多核一样，爬虫最终还是要走向多线程乃至分布式。尤其是待爬取的任务\n",
    "            工作量大而又重复度很高时，就非常适用多线程爬虫，相当于从一只虫子变成了多只虫子在爬，效率提升明显。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 【0】爬虫伪装技巧\n",
    "1.浏览器伪装\n",
    "因为网站服务器能够很轻易的识别出访问的来源浏览器，以requests请求为例，默认header头数据中没有浏览器信息，在与浏览器交互时简直就是“裸奔”，所以我们可以加入“User-Agent”信息伪装成真实浏览器，代码如下：\n",
    "\n",
    "import requests \n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0'}  #模拟成火狐浏览器\n",
    "response = requests.get(\"http://www.baidu.com\",headers=headers)  #模拟请求url\n",
    "# ————————————————————————————\n",
    "2.访问地址伪装\n",
    "访问地址指的是headers头部中的reffer信息，那么它有什么作用呢？举个例子解释一下：\n",
    "我在https://bj.meituan.com/里有一个https://waimai.meituan.com/链接，那么点击这个https://waimai.meituan.com/，\n",
    "它的header信息里就有：Referer=https://bj.meituan.com/ \n",
    "那么可以利用这个来防止盗链，比如我只允许我自己的网站访问我自己的图片服务器\n",
    "我们可以加入“reffer”信息伪装访问地址，代码如下：\n",
    "\n",
    "import requests \n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    'reffer':'https://bj.meituan.com/'}\n",
    "response = requests.get(\"https://waimai.meituan.com/\",headers=headers)  #模拟请求url\n",
    "# ————————————————————————————\n",
    "\n",
    "3.ip地址伪装\n",
    "对于网络中的反爬虫策略来说，大多数都是根据单个IP的行为来判断是不是网络爬虫的，例如，反爬虫检测到某个IP的\n",
    "访问次数很多，或者是访问的频率很快，就会封禁这个IP。这时我们就要选择代理IP来突破反爬虫的机制，更稳定的及逆行数据\n",
    "的爬取。代理IP可以自己去网上找免费的，但不太稳定，也可去花钱买一些比较稳定的。\n",
    "python添加代理IP的代码如下：\n",
    "\n",
    "import requests \n",
    "proxies={'https':'101.236.54.97:8866'} \n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    'reffer':'https://bj.meituan.com/'}\n",
    "response = requests.get(\"https://waimai.meituan.com/\",headers=headers，proxies=proxies)  #模拟请求url\n",
    "# ————————————————————————————\n",
    "4.伪装访问速率\n",
    "真实用户的访问次数以及访问规律是很稳定的，并不会多次的访问，所以我们要伪装成真实的用户来爬取数据，这样反爬虫机制\n",
    "就不会察觉，可以采用控制访问频率的方式，主要是随机设置访问时间，代码如下：\n",
    "\n",
    "import requests \n",
    "import time,random\n",
    "proxies={'https':'101.236.54.97:8866'} \n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    'reffer':'https://bj.meituan.com/'}\n",
    "for i in range(10):\n",
    "    response = requests.get(\"https://waimai.meituan.com/\",headers=headers，proxies=proxies)  #模拟请求url\n",
    "    time.sleep(random.uniform(1.1,5.4))\n",
    "\n",
    "# ————————————————————————————\n",
    "5.伪装用户真实信息\n",
    "有些网页是需要登录后才会显示数据，而cookie值会携带个人的登录信息，在爬虫中加入cookie值就能避免登录的麻烦，\n",
    "例如知乎、京东等网站，加入方法如下：\n",
    "\n",
    "import requests \n",
    "proxies={'https':'101.236.54.97:8866'} \n",
    "headers={\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    'reffer':'https://bj.meituan.com/'}\n",
    "cookies=''\n",
    "response = requests.get(\"https://waimai.meituan.com/\",headers=headers，proxies=proxies,,cookies=cookies)  #模拟请求url\n",
    "# ————————————————————————————\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 【1】requests + random + bs4 + re + lxml.etree 获取房价：\"http://www.ke.com\"\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import math\n",
    "from lxml import etree\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)\",\n",
    "    \"Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)\",\n",
    "    \"Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1\",\n",
    "    \"Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0\",\n",
    "    \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20\",\n",
    "    \"Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52\",]\n",
    "chinese_city_district_dict = dict()\n",
    "chinese_area_dict = dict() \n",
    "\n",
    "def create_headers():\n",
    "    headers = dict()\n",
    "    headers[\"User-Agent\"] = random.choice(USER_AGENTS)\n",
    "    headers[\"Referer\"] = \"http://www.ke.com\"\n",
    "    return headers\n",
    "class SecHouse(object):\n",
    "    def __init__(self, district, area, name, price, desc, pic):\n",
    "        self.district = district\n",
    "        self.area = area\n",
    "        self.price = price\n",
    "        self.name = name\n",
    "        self.desc = desc\n",
    "        self.pic = pic\n",
    "    def text(self):\n",
    "        return self.district + \",\" + \\\n",
    "                self.area + \",\" + \\\n",
    "                self.name + \",\" + \\\n",
    "                self.price + \",\" + \\\n",
    "                self.desc + \",\" + \\\n",
    "                self.pic\n",
    "def get_districts():\n",
    "    url = 'https://bj.ke.com/xiaoqu/'\n",
    "    headers = create_headers()\n",
    "    response = requests.get(url, timeout=10, headers=headers)\n",
    "    html = response.content\n",
    "    root = etree.HTML(html)\n",
    "    elements = root.xpath('//div[@data-role=\"ershoufang\"]/div/a')\n",
    "    en_names = list()\n",
    "    ch_names = list()\n",
    "    for element in elements:\n",
    "        link = element.attrib['href']\n",
    "        en_names.append(link.split('/')[-2])\n",
    "        ch_names.append(element.text)\n",
    "\n",
    "    # 打印区县英文和中文名列表\n",
    "    for index, name in enumerate(en_names):\n",
    "        chinese_city_district_dict[name] = ch_names[index]\n",
    "    return en_names\n",
    "\n",
    "def get_areas(district):\n",
    "    page = \"http://bj.ke.com/xiaoqu/{0}\".format(district)\n",
    "    areas = list()\n",
    "    try:\n",
    "        headers = create_headers()\n",
    "        response = requests.get(page, timeout=10, headers=headers)\n",
    "        html = response.content\n",
    "        root = etree.HTML(html)\n",
    "        links = root.xpath('//div[@data-role=\"ershoufang\"]/div[2]//a')\n",
    "        # 针对a标签的list进行处理\n",
    "        for link in links:\n",
    "            relative_link = link.attrib['href']\n",
    "            relative_link = relative_link[:-1]   # 去掉最后的\"/\"\n",
    "            area = relative_link.split(\"/\")[-1]  # 获取最后一节 英文县城\n",
    "            if area != district:                 # 去掉区县名,防止重复\n",
    "                chinese_area = link.text   # 获取中文县城名\n",
    "                chinese_area_dict[area] = chinese_area\n",
    "                # print(chinese_area)\n",
    "                areas.append(area)\n",
    "        return areas\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "'''_______________ 测试______________________'''    \n",
    "# districts = get_districts()\n",
    "# print(districts)                   # 城区的英文名列表\n",
    "# print(chinese_city_district_dict)  # 城区的中英文字典 'dongcheng': '东城'\n",
    "# get_area=get_areas(\"dongcheng\")    \n",
    "# print(get_area)              # 县城的英文名列表\n",
    "# print(chinese_area_dict)     # 县城的中英文字典  'andingmen': '安定门'\n",
    "'''_________________保存数据____________________'''\n",
    "with open(r\"C:\\Users\\hp\\Desktop\\sechouse.txt\", \"w\", encoding='utf-8') as f:\n",
    "# 开始获得需要的板块数据\n",
    "    total_page = 1\n",
    "    sec_house_list = list()\n",
    "    districts = get_districts()         # 城区的英文名列表\n",
    "    for district in districts[0:1]:\n",
    "        arealist = get_areas(district)  # 县城的英文名列表\n",
    "        for area in arealist[0:1]:\n",
    "# 中文区县: 城区的中英文字典 'dongcheng': '东城'\n",
    "            chinese_district = chinese_city_district_dict[district]\n",
    "#             print(chinese_district)\n",
    "# 中文版块: 县城的中英文字典  'andingmen': '安定门'\n",
    "            chinese_area = chinese_area_dict.get(area, \"\")\n",
    "#             print(chinese_area)\n",
    "            page = 'http://bj.ke.com/ershoufang/{0}/'.format(area)\n",
    "            headers = create_headers()\n",
    "            response = requests.get(page, timeout=10, headers=headers)\n",
    "            html = response.content\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "# # 获得总的页数\n",
    "            try:\n",
    "                page_box = soup.find_all('div', class_='page-box')[0]\n",
    "                matches = re.search('totalPage\":(\\d+)', str(page_box))\n",
    "                total_page = int(matches.group(1))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            print(total_page)\n",
    "# # 从第一页开始,一直遍历到最后一页\n",
    "            headers = create_headers()\n",
    "            for i in range(1, total_page + 1):\n",
    "                page = 'http://bj.ke.com/ershoufang/{0}/pg{1}'.format(area,i)\n",
    "                print(\"正在获取第\",page,\"页\")\n",
    "                response = requests.get(page, timeout=10, headers=headers)\n",
    "                html = response.content\n",
    "                soup = BeautifulSoup(html, \"lxml\")\n",
    "                # 获得有小区信息的panel\n",
    "                house_elements = soup.find_all('li', class_=\"clear\")\n",
    "                for house_elem in house_elements:\n",
    "                    price = house_elem.find('div', class_=\"totalPrice\")\n",
    "                    name = house_elem.find('div', class_='title')\n",
    "                    desc = house_elem.find('div', class_=\"houseInfo\")\n",
    "                    pic = house_elem.find('a', class_=\"img\").find('img', class_=\"lj-lazy\")\n",
    "# 继续清理数据\n",
    "                    price = price.text.strip()\n",
    "                    name = name.text.replace(\"\\n\", \"\")\n",
    "                    desc = desc.text.replace(\"\\n\", \"\").strip()\n",
    "                    pic = pic.get('data-original').strip()\n",
    "# # 作为对象保存\n",
    "                    sec_house = SecHouse(chinese_district, chinese_area, name, price, desc, pic)\n",
    "                    print(sec_house.text())\n",
    "                    sec_house_list.append(sec_house)   \n",
    "    for sec_house in sec_house_list:\n",
    "        f.write(sec_house.text() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 【2】Python爬取字节跳动1W+招聘信息：x-csrf-token\n",
    "import requests\n",
    "from urllib.parse import unquote\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "session = requests.session()\n",
    "page = 10\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36',\n",
    "    'Origin': 'https://jobs.bytedance.com',\n",
    "    'Referer': f'https://jobs.bytedance.com/experienced/position?keywords=&category=&location=&project=&type=&job_hot_flag=&current=1&limit={page}'}\n",
    "data = {\"portal_entrance\": 1}\n",
    "url = \"https://jobs.bytedance.com/api/v1/csrf/token\"\n",
    "r = session.post(url, headers=headers, data=data)\n",
    "cookies = session.cookies.get_dict()\n",
    "\n",
    "url = \"https://jobs.bytedance.com/api/v1/search/job/posts\"\n",
    "headers[\"x-csrf-token\"] = unquote(cookies[\"atsx-csrf-token\"])\n",
    "data = {\n",
    "    \"job_category_id_list\": [],\n",
    "    \"keyword\": \"\",\n",
    "    \"limit\": page,\n",
    "    \"location_code_list\": [],\n",
    "    \"offset\": 0,\n",
    "    \"portal_entrance\": 1,\n",
    "    \"portal_type\": 2,\n",
    "    \"recruitment_id_list\": [],\n",
    "    \"subject_id_list\": [] }\n",
    "for i in range(5):\n",
    "    print(f\"准备爬取第{i}页\")\n",
    "    data[\"offset\"] = i*page\n",
    "    r = None\n",
    "    while not r:\n",
    "        try:\n",
    "            r = session.post(url, headers=headers,data=json.dumps(data), timeout=3)\n",
    "        except Exception as e:\n",
    "            print(\"访问超时！等待5s\", e)\n",
    "            time.sleep(5)\n",
    "    df = pd.DataFrame(r.json()['data']['job_post_list'])\n",
    "    if df.shape[0] == 0:\n",
    "        print(\"爬取完毕！！！\")\n",
    "        break\n",
    "    df.city_info = df.city_info.str['name']\n",
    "    df.recruit_type = df.recruit_type.str['parent'].str['name']\n",
    "    tmp = []\n",
    "    for x in df.job_category.values:\n",
    "        if x['parent']:\n",
    "            tmp.append(f\"{x['parent']['name']}-{x['name']}\")\n",
    "        else:\n",
    "            tmp.append(x['name'])\n",
    "    df.job_category = tmp\n",
    "    df.publish_time = df.publish_time.apply(lambda x: pd.Timestamp(x, unit=\"ms\"))\n",
    "    df.drop(columns=['sub_title', 'job_hot_flag', 'job_subject'], inplace=True)\n",
    "    df.to_csv(r\"C:\\Users\\hp\\Desktop\\bytedance_jobs.csv\", mode=\"a\", encoding='utf_8_sig',\n",
    "              header=not os.path.exists(r\"C:\\Users\\hp\\Desktop\\bytedance_jobs.csv\"), index=False)\n",
    "    print(\",\".join(df.title.head(10)))\n",
    "# 对结果去重\n",
    "df = pd.read_csv(r\"C:\\Users\\hp\\Desktop\\bytedance_jobs.csv\",encoding='utf_8_sig',)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.to_csv(r\"C:\\Users\\hp\\Desktop\\bytedance_jobs.csv\", index=False,encoding='utf_8_sig',)\n",
    "print(\"共爬取\", df.shape[0], \"行无重复数据\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
