{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#  lxml简介：\n",
    "https://www.w3school.com.cn/xpath/index.asp  \n",
    "'''_________________选取若干路径_________________\n",
    "//book/title | //book/price\t选取 book 元素的所有 title 和 price 元素。\n",
    "//title | //price\t选取文档中的所有 title 和 price 元素。\n",
    "/bookstore/book/title | //price\t选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。\n",
    "'''\n",
    "'''_________________选取元素_________________\n",
    "bookstore\t选取 bookstore 元素的所有子节点。\n",
    "/bookstore\t选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！\n",
    "bookstore/book\t选取属于 bookstore 的子元素的所有 book 元素。\n",
    "//book\t选取所有 book 子元素，而不管它们在文档中的位置。\n",
    "bookstore//book\t选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。\n",
    "//@lang\t选取名为 lang 的所有属性。\n",
    "\n",
    "/bookstore/*\t选取 bookstore 元素的所有子元素。\n",
    "//*\t选取文档中的所有元素。\n",
    "//title[@*]\t选取所有带有属性的 title 元素。\n",
    "\n",
    "/bookstore/book[1]\t选取属于 bookstore 子元素的第一个 book 元素。\n",
    "/bookstore/book[last()]\t选取属于 bookstore 子元素的最后一个 book 元素。\n",
    "/bookstore/book[last()-1]\t选取属于 bookstore 子元素的倒数第二个 book 元素。\n",
    "/bookstore/book[position()<3]\t选取最前面的两个属于 bookstore 元素的子元素的 book 元素。\n",
    "//title[@lang]\t选取所有拥有名为 lang 的属性的 title 元素。\n",
    "//title[@lang='eng']\t选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。\n",
    "/bookstore/book[price>35.00]\t选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。\n",
    "/bookstore/book[price>35.00]/title\t选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。\n",
    "'''\n",
    "\n",
    "'''_________________XPath 轴_________________\n",
    "child::book\t选取所有属于当前节点的子元素的 book 节点。\n",
    "attribute::lang\t选取当前节点的 lang 属性。\n",
    "child::*\t选取当前节点的所有子元素。\n",
    "attribute::*\t选取当前节点的所有属性。\n",
    "child::text()\t选取当前节点的所有文本子节点。\n",
    "child::node()\t选取当前节点的所有子节点。\n",
    "descendant::book\t选取当前节点的所有 book 后代。\n",
    "ancestor::book\t选择当前节点的所有 book 先辈。\n",
    "ancestor-or-self::book\t选取当前节点的所有 book 先辈以及当前节点（如果此节点是 book 节点）\n",
    "child::*/child::price\t选取当前节点的所有 price 孙节点。\n",
    "'''\n",
    "# ——————————————————————————————————————————\n",
    "'''\n",
    "html = lxml.etree.HTML(text)\n",
    "#使用text构造一个XPath解析对象,etree模块可以自动修正HTML文本\n",
    "html = lxml.etree.parse('./ex.html',etree.HTMLParser())\n",
    "#直接读取文本进行解析\n",
    "from lxml import etree\n",
    "result = html.xpath('//*')\n",
    "#选取所有节点\n",
    "result = html.xpath('//li')\n",
    "#获取所有li节点\n",
    "result = html.xpath('//li/a')\n",
    "#获取所有li节点的直接a子节点\n",
    "result = html.xpath('//li//a')\n",
    "#获取所有li节点的所有a子孙节点\n",
    "result = html.xpath('//a[@href=\"link.html\"]/../@class')\n",
    "#获取所有href属性为link.html的a节点的父节点的class属性\n",
    "result = html.xpath('//li[@class=\"ni\"]')\n",
    "#获取所有class属性为ni的li节点\n",
    "result = html.xpath('//li/text()')\n",
    "#获取所有li节点的文本\n",
    "result = html.xpath('//li/a/@href')\n",
    "#获取所有li节点的a节点的href属性\n",
    "result = html.xpath('//li[contains(@class,\"li\")]/a/text())\n",
    "#当li的class属性有多个值时，需用contains函数完成匹配\n",
    "result = html.xpath('//li[contains(@class,\"li\") and @name=\"item\"]/a/text()')\n",
    "#多属性匹配\n",
    "result = html.xpath('//li[1]/a/text()')\n",
    "result = html.xpath('//li[last()]/a/text()')\n",
    "result = html.xpath('//li[position()<3]/a/text()')\n",
    "result = html.xpath('//li[last()-2]/a/text()')\n",
    "#按序选择，中括号内为XPath提供的函数\n",
    "result = html.xpath('//li[1]/ancestor::*')\n",
    "#获取祖先节点\n",
    "result = html.xpath('//li[1]/ancestor::div')\n",
    "result = html.xpath('//li[1]/attribute::*')\n",
    "#获取属性值\n",
    "result = html.xpath('//li[1]/child::a[@href=\"link1.html\"]')\n",
    "#获取直接子节点\n",
    "result = html.xpath('//li[1]/descendant::span')\n",
    "#获取所有子孙节点\n",
    "result = html.xpath('//li[1]/following::*[2]')\n",
    "#获取当前节点之后的所有节点的第二个\n",
    "result = html.xpath('//li[1]/following-sibling::*')\n",
    "#获取后续所有同级节点\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （0）lxml `基础_1：·保存html到本地、从本地读取html，tostring、/ //  、 .. ，@  [@*] \n",
    "'''\n",
    "nodename\t选取此节点的所有子节点\n",
    "/\t从当前节点选区直接子节点\n",
    "//\t从当前节点选取子孙节点\n",
    ".\t选取当前节点\n",
    "..\t选取当前节点的父节点\n",
    "@\t选取属性\n",
    "'''\n",
    "from lxml import etree\n",
    "\n",
    "'''将html写成文件保存到本地'''\n",
    "# text = '''\n",
    "# <div>\n",
    "# <ul>\n",
    "# <li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n",
    "# <li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n",
    "# <li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n",
    "# <li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n",
    "# <li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n",
    "# </ul>\n",
    "# </div>\n",
    "# '''\n",
    "# with open('C:\\\\Users\\\\hp\\\\Desktop\\\\text.html','w') as f:\n",
    "#     f.write(text)\n",
    "\n",
    "'''首先导入 lxml 库的 etree 模块，然后声明一段 HTML 文本，调用 HTML 类进行初始化，成功构造一个 XPath 解析对象。\n",
    "注意：HTML 文本中最后一个 li 节点没有闭合，但是 etree 模块可以自动修正 HTML 文本。\n",
    "调用 tostring() 方法即可输出修正后的 HTML 代码，但结果是 bytes 类型，可以用 decode() 方法将其转化为 str 类型，\n",
    "结果如下'''\n",
    "# html = etree.parse(r'C:\\Users\\hp\\Desktop\\text.html', etree.HTMLParser())\n",
    "# result = etree.tostring(html)\n",
    "# print(result.decode('utf-8'))\n",
    "'''属性获取/文本获取/知道子节点，查询父节点'''\n",
    "html = etree.parse(r'C:\\Users\\hp\\Desktop\\text.html', etree.HTMLParser())\n",
    "# result = html.xpath('//li/a/@href')       # 属性获取\n",
    "# print(result)\n",
    "# result = html.xpath('//*/text()')         # 文本获取\n",
    "# print(result)\n",
    "# result = html.xpath('//li[@class=\"item-0\"]/a/text()') # 一是获取文本所在节点后直接获取文本\n",
    "# print(result)\n",
    "# result = html.xpath('//li[@class=\"item-0\"]//text()')  # 二是使用 //，会获取到补全代码时换行产生的特殊字符\n",
    "# print(result)\n",
    "# result = html.xpath('//a[@href=\"link4.html\"]/../@class')  #  知道子节点，查询父节点可以用 .. 来实现：\n",
    "# print(result)\n",
    "'''多属性匹配'''\n",
    "# text = '''<li class=\"li li-first\" name=\"item\"><a href=\"link.html\">first item</a></li>'''\n",
    "# html = etree.HTML(text)\n",
    "# result = html.xpath('//li[contains(@class, \"li\") and @name=\"item\"]/a/@href')  # 多属性匹配\n",
    "# print(result)\n",
    "# result = html.xpath('//li[contains(@class, \"li\") and @name=\"item\"]/a/text()') # 多属性匹配\n",
    "# print(result)\n",
    "# result = html.xpath('//li[contains(@class, \"li\")]/a/text()')  # 属性多值匹配\n",
    "# print(result)\n",
    "\n",
    "''''''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （0）lxml 基础_2：按序选择 last、position、last; 节点轴选择 ancestor、attribute、child、following、descendant\n",
    "from lxml import etree\n",
    "text = '''\n",
    "<div>\n",
    "<ul>\n",
    "<li class=\"item-0\"><a href=\"link1.html\">first item</a></li>\n",
    "<li class=\"item-1\"><a href=\"link2.html\">second item</a></li>\n",
    "<li class=\"item-inactive\"><a href=\"link3.html\">third item</a></li>\n",
    "<li class=\"item-1\"><a href=\"link4.html\">fourth item</a></li>\n",
    "<li class=\"item-0\"><a href=\"link5.html\">fifth item</a>\n",
    "</ul>\n",
    "</div>\n",
    "'''\n",
    "html = etree.HTML(text)\n",
    "'''____________________ 按序选择 last、position、last _____________________'''\n",
    "# result = html.xpath('//li[1]/a/text()')       # 获取第一个\n",
    "# print(result)\n",
    "# result = html.xpath('//li[last()]/a/text()')  # 获取最后一个\n",
    "# print(result)\n",
    "# result = html.xpath('//li[position()<3]/a/text()') # 获取前两个\n",
    "# print(result)\n",
    "# result = html.xpath('//li[last()-2]/a/text()') # 获取倒数第三个\n",
    "# print(result)\n",
    "'''____________________ 节点轴选择 _____________________'''\n",
    "result = html.xpath('//li[1]/ancestor::*')    # 获取所有祖先节点\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/ancestor::div')  # 获取 div 祖先节点\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/attribute::*')  # 获取当前节点所有属性值\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/child::a[@href=\"link1.html\"]')  # 获取 href 属性值为 link1.html 的直接子节点\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/descendant::span')  # 获取所有的的子孙节点中包含 span 节点但不包含 a 节点\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/following::*[2]')   # 获取当前所有节点之后的第二个节点\n",
    "print(result)\n",
    "result = html.xpath('//li[1]/following-sibling::*') # 获取当前节点之后的所有同级节点\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （一）下载天堂网图片：requests + lxml.etree(Xpath) + fake_useragent.UserAgent + time + random\n",
    "import requests\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "class TianTangWebsite(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.ivsky.com/bizhi/1920x1080/index_{}.html\"\n",
    "        # 初始化url地址 重构构造头\n",
    "        self.film_list = []\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        for i in range(1, 10):\n",
    "            self.headers = {'User-Agent': ua.random}\n",
    "#             print(self.headers)\n",
    "            \n",
    "    def get_home(self, url):\n",
    "        html = requests.get(url=url, headers=self.headers).content.decode(\"utf-8\")\n",
    "#         print(html)\n",
    "        self.xiap(html)      # 请求一级页面的网站\n",
    "\n",
    "    def xiap(self,html):\n",
    "        h = etree.HTML(html) # 将获得的一级页面响应装入 etree\n",
    "        ha = h.xpath(\"//ul[@class='ali']//li/div//a//@href\")    # 用 xpath 解释一级页面\n",
    "        for a in ha:\n",
    "            q=\"https://www.ivsky.com\" + a\n",
    "            self.tow(q)\n",
    "\n",
    "    def tow(self,q):\n",
    "        html1 = requests.get(url=q, headers=self.headers).content.decode(\"utf-8\")\n",
    "        self.tow_get(html1)\n",
    "\n",
    "    def tow_get(self,html1):\n",
    "        qe= etree.HTML(html1) \n",
    "        ha =qe.xpath(\"//ul[@class='pli']//li/div/a/img/@src\")\n",
    "        ht =qe.xpath(\"//ul[@class='pli']//li/div/a/img/@alt\")\n",
    "        print(ht[0])\n",
    "        for i in ha:\n",
    "            w=\"https:\"+i\n",
    "            pichtml = requests.get(url=w, headers=self.headers).content\n",
    "            filename =\"C:\\\\Users\\\\hp\\\\Desktop\\\\天堂网爬的图片\\\\\"\n",
    "            filename = filename + ht[0]+w[-6:]\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(pichtml)\n",
    "                print(\"%s下载成功\" % filename)\n",
    "\n",
    "    def main(self):\n",
    "        for i in range(1,2):          #页数随机客户随便 设置\n",
    "            url=self.url.format(i)    # https://www.ivsky.com/bizhi/1920x1080/index_4.html 第4页\n",
    "            print(url)\n",
    "            html=self.get_home(url)\n",
    "#             time.sleep(random.randint(1, 3))\n",
    "#             print('第%d提取成功！！！' % i)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spider =TianTangWebsite()\n",
    "    spider.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （一）获取素材图片：requests + lxml —— https://www.51miz.com/\n",
    "import requests\n",
    "from lxml import etree\n",
    "from pprint import pprint\n",
    "import os\n",
    "filepath=os.getcwd()\n",
    "os.makedirs(filepath+\"\\\\\"+ \"Download_Picture\")\n",
    "filepath=filepath+\"\\\\\"+ \"Download_Picture\"\n",
    "class ImageSpider(object):\n",
    "    def __init__(self):\n",
    "        self.firsr_url = \"https://www.51miz.com/so-sucai/1789243-13-0-0-default/p_{}/\"\n",
    "        self.headers = {\n",
    "            \"User-Agent\":\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\"\n",
    "        }\n",
    "    def get_page(self, url):\n",
    "        res = requests.get(url=url, headers=self.headers)\n",
    "        html = res.content.decode('utf-8')\n",
    "        return html\n",
    "    def parse_page(self, html):\n",
    "        parse_html = etree.HTML(html)\n",
    "        image_src_list = parse_html.xpath('//a[@class=\"image-box\"]/@href')\n",
    "        for image_src in image_src_list:\n",
    "#             print(image_src)\n",
    "            html1 = self.get_page(image_src)\n",
    "            parse_html1 = etree.HTML(html1)\n",
    "            bimg_url = parse_html1.xpath('//img[@class=\"previewPic previewPic_0 previewPicSmall_0 show\"]/@src')[0]\n",
    "            filename = parse_html1.xpath('//h1[@class=\"iftip\"]/text()')[0]\n",
    "            dirname = filepath + '\\\\' + filename + '.jpg'\n",
    "            html2 = requests.get(url=\"http:\" + bimg_url,headers=self.headers).content\n",
    "            with open(dirname, 'wb') as f:\n",
    "                f.write(html2)\n",
    "                print(\"%s下载成功\" % filename)\n",
    "    def main(self):\n",
    "        startPage = int(input(\"起始页:\"))\n",
    "        endPage = int(input(\"终止页:\"))\n",
    "        for page in range(startPage, endPage + 1):\n",
    "            url = self.firsr_url.format(page)\n",
    "            html = self.get_page(url)\n",
    "            self.parse_page(html)\n",
    "if __name__ == '__main__':\n",
    "    imageSpider = ImageSpider()\n",
    "    imageSpider.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）获得天气信息_1：requests + get + lxml.etree(Xpath) + pandas + re  —— http://www.weather.com.cn/\n",
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "url = 'http://www.weather.com.cn/weather1d/101010100.shtml#input'\n",
    "with requests.get(url) as res:\n",
    "    content = res.content.decode(encoding='utf-8')\n",
    "    html = etree.HTML(content)\n",
    "location = html.xpath('//*[@id=\"around\"]//a[@target=\"_blank\"]/span/text()')\n",
    "temperature = html.xpath('//div[@id=\"around\"]//li//i/text()')\n",
    "df = pd.DataFrame({'location':location, 'temperature':temperature})\n",
    "display(df.head(8))\n",
    "'''——————————————(1)————————————————'''\n",
    "# df['high'] = df['temperature'].apply(lambda x: int(re.match('(-?[0-9]*?)/-?[0-9]*?°C', xabs).group(1) ) )\n",
    "# df['low'] = df['temperature'].apply(lambda x: int(re.match('-?[0-9]*?/(-?[0-9]*?)°C', x).group(1) ) )\n",
    "'''——————————————(2)————————————————'''\n",
    "# df['high'] = df['temperature'].apply(lambda x: int(re.match('(-?[0-9]*?)/(-?[0-9]*?)°C', x).group(1) ) )\n",
    "# df['low'] = df['temperature'].apply(lambda x: int(re.match('(-?[0-9]*?)/(-?[0-9]*?)°C', x).group(2) ) )\n",
    "'''——————————————(3)————————————————'''\n",
    "# df['high'] = df['temperature'].apply(lambda x: int(re.findall('(\\d{1,2})', x)[0] ) )\n",
    "# df['low'] = df['temperature'].apply(lambda x: int(re.findall('(\\d{1,2})', x)[1]) ) \n",
    "'''——————————————————————————————'''\n",
    "# display(df.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）获得天气信息_2： urllib.request +  Request + urlopen + read —— http://www.weather.com.cn/\n",
    "import urllib.request\n",
    "url = 'http://www.weather.com.cn/weather1d/101010100.shtml#input'\n",
    "'''——————————————————————————'''\n",
    "# response = urllib.request.urlopen(url)\n",
    "'''——————————————————————————'''\n",
    "req = urllib.request.Request(url)\n",
    "response = urllib.request.urlopen(req)\n",
    "'''——————————————————————————'''\n",
    "html=response.read().decode('utf-8')\n",
    "print(html)\n",
    "# with open(r'C:\\Users\\hp\\Desktop\\2.txt','wb') as of:\n",
    "#     of.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （二）获得天气信息_3：requests + soupparser.fromstring + etree.tostring —— http://www.weather.com.cn/\n",
    "import requests\n",
    "import lxml.html.soupparser as soupparser\n",
    "url = 'http://www.weather.com.cn/weather1d/101010100.shtml#input'\n",
    "with requests.get(url) as res:\n",
    "    content = res.content.decode(encoding='utf-8')\n",
    "    demo = soupparser.fromstring(content)\n",
    "    html= etree.tostring(demo,encoding='utf-8',pretty_print=True)\n",
    "    print(html.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （三） requests + lxml.etree + pandas.csv + time：爬取鲁迅先生《经典语录》—— http://www.shuoshuodaitupian.com\n",
    "import requests\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import time\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" \\\n",
    "                         \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\",}\n",
    "# 保存数据\n",
    "item_list = []\n",
    "for i in range(0, 2):\n",
    "    # 1、获取数据\n",
    "    url = \"http://www.shuoshuodaitupian.com/writer/128_\" + str(i + 1) # 1-10页\n",
    "    result = requests.get(url, headers=headers).content.decode()\n",
    "    # 2、解析数据\n",
    "    html = etree.HTML(result)\n",
    "    div_list = html.xpath('//div[@class=\"item statistic_item\"]')\n",
    "    div_list = div_list[1:-1]\n",
    "\n",
    "    for div in div_list:  # 遍历每一条信息\n",
    "        item = {}\n",
    "        # ./ 注意从当前节点，向下获取\n",
    "        item['content'] = div.xpath('./a/text()')[0]\n",
    "        item['source'] = div.xpath('./div[@class=\"author_zuopin\"]/text()')[0]\n",
    "        item['score'] = div.xpath('.//a[@class=\"infobox zan like \"]/span/text()')[0]\n",
    "        item_list.append(item)\n",
    "    print(\"正在爬取第{}页\".format(i + 1))\n",
    "    time.sleep(0.1)\n",
    "# 3、保存数据\n",
    "# df = pd.DataFrame(item_list) \n",
    "# df.to_csv('./鲁迅经典语录.csv', encoding='utf_8_sig')   # 保证不乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （四）requests + lxml.etree· + fake_useragent 获取Mikan动漫视频资源 https://mikanani.me/Home/Classic\n",
    "import requests\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "class Mikan(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"https://mikanani.me/Home/Classic/{}\"  # /zhuanchang/:搜索的名字的拼音缩写\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        self.headers = {'User-Agent': ua.random}\n",
    "    def get_page(self, url):\n",
    "        req = requests.get(url=url, headers=self.headers)\n",
    "        html = req.content.decode(\"utf-8\")\n",
    "        return html\n",
    "    def page_page(self, html):\n",
    "        parse_html = etree.HTML(html)\n",
    "        one = parse_html.xpath('//tbody//tr//td[3]/a/@href')\n",
    "        for li in one:\n",
    "            yr = \"https://mikanani.me\" + li\n",
    "            html2 = self.get_page(yr)  # 第二个发生请求\n",
    "            parse_html2 = etree.HTML(html2)\n",
    "            tow = parse_html2.xpath('//body')\n",
    "            for i in tow:\n",
    "                four = i.xpath('.//p[@class=\"episode-title\"]//text()')[0].strip()\n",
    "                fif = i.xpath('.//div[@class=\"leftbar-nav\"]/a[1]/@href')[0].strip()\n",
    "                print(four)\n",
    "                t = \"https://mikanani.me\" + fif\n",
    "                print(t)\n",
    "#                 dirname = \"./种子/\" + four[:15] + four[-20:] + '.torrent'\n",
    "#                 # print(dirname)\n",
    "#                 html3 = requests.get(url=t, headers=self.headers).content\n",
    "#                 with open(dirname, 'wb') as f:\n",
    "#                     f.write(html3)\n",
    "#                     print(\"\\n%s下载成功\" % four)\n",
    "\n",
    "    def main(self):\n",
    "        stat = int(input(\"start :\"))\n",
    "        end = int(input(\" end:\"))\n",
    "        for page in range(stat, end + 1):\n",
    "            url = self.url.format(page)\n",
    "            html = self.get_page(url)\n",
    "            self.page_page(html)\n",
    "            print(\"==================第%s页爬取成功！！！！=====================\" % page)\n",
    "if __name__ == '__main__':\n",
    "    Siper = Mikan()\n",
    "    Siper.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （五）requests + lxml.etree + fake_useragent + time + txt 保存数据 获取菜谱信息 https://www.xiachufang.com\n",
    "import requests\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "class kitchen(object):\n",
    "    u = 0;\n",
    "    def __init__(self):\n",
    "        self.url = \"https://www.xiachufang.com/explore/?page={}\"\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        for i in range(1, 50):\n",
    "            self.headers = {'User-Agent': ua.random,}\n",
    "    def get_page(self, url):\n",
    "        res = requests.get(url=url, headers=self.headers)\n",
    "        html = res.content.decode(\"utf-8\")\n",
    "        return html\n",
    "    def parse_page(self, html):\n",
    "        parse_html = etree.HTML(html)\n",
    "        image_src_list = parse_html.xpath('//li/div/a/@href')\n",
    "        for i in image_src_list:\n",
    "            url = \"https://www.xiachufang.com/\" + i\n",
    "            html1 = self.get_page(url)       # 第二个发生请求\n",
    "            parse_html1 = etree.HTML(html1)\n",
    "            num = parse_html1.xpath('.//h2[@id=\"steps\"]/text()')[0].strip()\n",
    "            name = parse_html1.xpath('.//li[@class=\"container\"]/p/text()')\n",
    "            ingredients = parse_html1.xpath('.//td//a/text()')\n",
    "            self.u += 1;\n",
    "            food_info = '''  \n",
    "第 %s 种\n",
    "        \n",
    "菜 名 : %s\n",
    "原 料 : %s\n",
    "下 载 链 接 : %s\n",
    "================================================================= ''' % (str(self.u), num, ingredients, url)\n",
    "#             print(food_info)\n",
    "            with open(r'C:\\Users\\hp\\Desktop\\菜谱.txt', 'a',encoding='utf-8') as f:\n",
    "                f.write(food_info)\n",
    "    def main(self):\n",
    "        startPage = int(input(\"起始页:\"))\n",
    "        endPage = int(input(\"终止页:\"))\n",
    "        for page in range(startPage, endPage + 1):\n",
    "            url = self.url.format(page)\n",
    "            html = self.get_page(url)\n",
    "            self.parse_page(html)\n",
    "            time.sleep(1.4)\n",
    "            print(\"==============================  第 %s 页 爬 取 成 功 = =========================\" % page)\n",
    "if __name__ == '__main__':\n",
    "    imageSpider = kitchen()\n",
    "    imageSpider.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （六）requests + lxml.etree + csv + fake_useragent + time 获取穷游旅游信息 https://place.qyer.com\n",
    "import requests\n",
    "from lxml import etree\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import csv\n",
    "class Travel(object):\n",
    "    def __init__(self):\n",
    "        self.url = \"https://place.qyer.com/south-korea/citylist-0-0-{}/\"\n",
    "        ua = UserAgent(verify_ssl=False)\n",
    "        for i in range(1, 50):\n",
    "            self.headers = {'User-Agent': ua.random}\n",
    "    def get_page(self, url):\n",
    "        res = requests.get(url=url, headers=self.headers)\n",
    "        html = res.content.decode(\"utf-8\")\n",
    "        return html\n",
    "    def parse_page(self, html):\n",
    "        parse_html = etree.HTML(html)\n",
    "        image_src_list = parse_html.xpath('//ul[@class=\"plcCitylist\"]/li')\n",
    "        csv_file = open(r'C:\\Users\\hp\\Desktop\\Cfy\\scrape.csv', 'a', encoding='gbk')\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['城市', '景点热度', '图片链接'])\n",
    "        for i in image_src_list:\n",
    "            b = i.xpath('.//h3//a/text()')[0].strip()\n",
    "            c = i.xpath('.//p[@class=\"beento\"]//text()')[0].strip()\n",
    "            d = i.xpath('.//p[@class=\"pics\"]//img//@src')[0].strip()\n",
    "            csv_writer.writerow([b, c, d])\n",
    "        csv_file.close()\n",
    "    def main(self):\n",
    "        startPage = int(input(\"起始页:\"))\n",
    "        endPage = int(input(\"终止页:\"))\n",
    "        for page in range(startPage, endPage + 1):\n",
    "            url = self.url.format(page)\n",
    "            html = self.get_page(url)\n",
    "            self.parse_page(html)\n",
    "            time.sleep(2)\n",
    "            print(\"======================第%s页爬取成功！！！！=======================\" % page)\n",
    "if __name__ == '__main__':\n",
    "    imageSpider = Travel()\n",
    "    imageSpider.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （0）pandas \n",
    "import pandas as pd\n",
    "# aa=[{'name':'张3','year':'10'},{'name':'张4','year':'11'},{'name':'张5','year':'12'}]\n",
    "# bb=[{'name':'李4','year':'13'},{'name':'李5','year':'14'},{'name':'李6','year':'15'}]\n",
    "# cc=[]\n",
    "# cc.extend(aa)\n",
    "# cc.extend(bb)\n",
    "# df=pd.DataFrame(cc)\n",
    "'''——————————————————'''\n",
    "dd={'name':['张3','张4','张5'],'year':[13,14,15]}\n",
    "df=pd.DataFrame(dd)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# （0）正则: compile、match、groups、group、findall、finditer\n",
    "import re\n",
    "str1='28/35°C'\n",
    "str2='8/35°C'\n",
    "'''————————————————————'''\n",
    "# result3=re.compile('(\\d{1,2})',re.I).findall(str2)\n",
    "# print(result3[0])\n",
    "# print(result3[1])\n",
    "'''————————————————————'''\n",
    "result2=re.compile('(-?[0-9]+)/(-?[0-9]+)°C',re.I).match(str1)\n",
    "print(result2.groups())\n",
    "print(result2.groups()[0])\n",
    "print(result2.groups()[1])\n",
    "\n",
    "# print(result2.group(1))\n",
    "# print(result2.group(2))\n",
    "'''————————————————————'''\n",
    "# result0=re.compile('(\\d{2})',re.I).findall( str1)\n",
    "# print(result0)\n",
    "'''————————————————————'''\n",
    "# result1=re.findall( '(\\d{2})',str1)\n",
    "# print(result1)\n",
    "'''————————————————————'''\n",
    "# result=re.finditer('(\\d{2})', str1)\n",
    "# for r in result:\n",
    "#     print(r.group())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
